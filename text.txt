	This report details the process of an engineering project whose aim was optimizing an AI program which would learn on its own how to drive around virtual tracks in the open source environment of SuperTuxKartTM in order to demonstrate understanding of information learned in the Boston University course EC418: Reinforcement Learning. The main goal of this project was to explore and document different approaches of utilizing either reinforcement or machine learning in order to improve the performance of existing code which already allowed the kart to complete the tracks in decent relative time. Understanding which approaches work and sharing them with other groups working on this project provides a solid foundation for understanding how and why certain approaches are utilized and how they function.
	The main approach chosen by for this project for this group was an RL implementation in two parts: Double Q Learning acting as the controller for the kart's actions, and Deep Deterministic Policy Gradient (DDPG) for identifying the track frame by frame for the kart to understand its position in the virtual environment and predict an aimpoint further down the track. The controller can utilize this aimpoint to guide its actions.
	A prior implementation of Q Learning was given to be utilized with the controller, but Double Q Learning was found to be more effective at both training the kart to maneuver the track faster and learn how to do so in a shorter period of time. The issue was reliability with test results for both regular and Double Q Learning which will be discussed in the Results section. Otherwise, a full RL implementation was optimized by Double Q Learning, but does not surpass a purely RL approach at the current level of implementation.
	For the planner, an approach which could utilize a continuous action space was preferred. DDPG was implemented using Actor-Critic methods as well as experience replay in an aim to generate aimpoints that can predict the desired track location. This approach faced multiple significant challenges including neural network architecture, interfacing with existing code and defining the parameters of the algorithm, e.g the reward function. We were able to solve these issues and implement an approach that runs but it is not able to learn fast enough. At this point, DDPG is not able to learn fast enough to complete the tracks in under 1000 seconds.
